[model]
arch = "vanilla_transformer"
tokenizer = "tokenizer.json"
max_len = 1024

[model.hparams]
n_layers = 6
d_model = 512
n_heads = 8
d_ff = 2048
dropout = 0.1

[train]
n_epochs = 10
batch_size = 32
lr = 3e-4
betas = [0.9, 0.98]
grad_accum_steps = 8
max_grad_norm = 1.0

[train.wandb]
log_interval = 100
entity = ""
project = "deep-learning-scratch"
