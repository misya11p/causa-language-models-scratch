{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf0acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fa051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "batch_size = 32\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe92f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.scale = d ** 0.5\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        weights = q @ k.mT\n",
    "        causal_mask = self._get_causal_mask(q.shape[-2])\n",
    "        weights = weights.masked_fill(causal_mask, -torch.inf)\n",
    "        scores = self.softmax(weights / self.scale)\n",
    "        out = scores @ v\n",
    "        return out\n",
    "\n",
    "    def _get_causal_mask(self, seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        return mask\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        assert not d_model % n_heads, \"d_model must be divisible by n_heads\"\n",
    "        d_k = d_model // n_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.attention = Attention(d_k)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=bias)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "        queries = torch.stack(q.chunk(self.n_heads, -1), 0)\n",
    "        keys = torch.stack(k.chunk(self.n_heads, -1), 0)\n",
    "        values = torch.stack(v.chunk(self.n_heads, -1), 0)\n",
    "        heads = self.attention(queries, keys, values).unbind(0)\n",
    "        out = self.w_o(torch.cat(heads, -1))\n",
    "        return out\n",
    "    \n",
    "    def _get_causal_mask(self, seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f325aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, n_heads, d_ff = 512, 8, 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014d31f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050624\n",
      "1050624\n"
     ]
    }
   ],
   "source": [
    "mha = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "n_params = sum(p.numel() for p in mha.parameters() if p.requires_grad)\n",
    "print(n_params)\n",
    "\n",
    "custom_mha = MultiHeadAttention(d_model, n_heads)\n",
    "n_custom_params = sum(p.numel() for p in custom_mha.parameters() if p.requires_grad)\n",
    "print(n_custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9b9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512]) torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len = 8, 10\n",
    "x = torch.randn(2, 10, d_model)\n",
    "out1, _ = mha(x, x, x)\n",
    "out2 = custom_mha(x)\n",
    "print(out1.shape, out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e080eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFowardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc2(self.relu(self.fc1(x)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667f15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = FeedFowardNetwork(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = self.mha(x)\n",
    "        z1 = self.norm1(z1 + x)\n",
    "        z2 = self.ffn(z1)\n",
    "        z2 = self.norm2(z2 + z1)\n",
    "        return z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdcc4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3152384\n",
      "3152384\n"
     ]
    }
   ],
   "source": [
    "transformer_encoder = nn.TransformerEncoderLayer(d_model, n_heads, d_ff, batch_first=True)\n",
    "n_params = sum(p.numel() for p in transformer_encoder.parameters() if p.requires_grad)\n",
    "print(n_params)\n",
    "\n",
    "transformer_layer = TransformerLayer(d_model, n_heads, d_ff)\n",
    "n_params = sum(p.numel() for p in transformer_layer.parameters() if p.requires_grad)\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "064ed23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers, d_model, n_heads, d_ff, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = nn.Parameter(torch.rand((max_len, d_model)))\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, n_heads, d_ff) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        seq_len = x.shape[1]\n",
    "        x = x + self.pe[:seq_len, :]\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9b5c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24290304\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "n_layers = 6\n",
    "max_len = 500\n",
    "model = Transformer(vocab_size, n_layers, d_model, n_heads, d_ff, max_len)\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b6b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24290304\n"
     ]
    }
   ],
   "source": [
    "model = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(d_model, n_heads, d_ff, batch_first=True),\n",
    "    num_layers=n_layers\n",
    ")\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n_params_embedding = vocab_size * d_model + max_len * d_model\n",
    "print(n_params + n_params_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3068d93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ae6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
